{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "926f2396",
   "metadata": {},
   "source": [
    "# Complete code for creation of catalogue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2a847d",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7169dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astroquery.gaia import Gaia\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import shutil\n",
    "import warnings\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from astropy.coordinates import SkyCoord\n",
    "import astropy.units as u\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277a65ae",
   "metadata": {},
   "source": [
    "Enter to Gaia portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5956fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_name = 'slastra'\n",
    "pass_gaia = '##########'\n",
    "\n",
    "Gaia.login(user = user_name, password= pass_gaia) # Also: Gaia.login()\n",
    "\n",
    "path = \"/home/santiago/Documents/Tesis/codeComplete\"\n",
    "\n",
    "#Threshold to remove outliers from very small or very large clusters.\n",
    "\n",
    "lim1_cmd=30\n",
    "lim2_cmd=900"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df2a635",
   "metadata": {},
   "source": [
    "## Functions necessary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c41c64",
   "metadata": {},
   "source": [
    "\"$\\textit{GAIA_search}$\" search all the stars locate it 1 degree around the center of the cluster that have $ruwe <1.4$, $parallax\\_over\\_error >8$, $parallax>0$ and $phot\\_g\\_mean\\_mag<20$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de97b3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAIA_search(clust):\n",
    "    radius  = 1    # Degrees\n",
    "    inp_ra  = hunt[hunt[\"name\"]==clust][\"ra\"].values[0]  # Degrees\n",
    "    inp_dec = hunt[hunt[\"name\"]==clust][\"dec\"].values[0]  # Degrees\n",
    "\n",
    "    query = f\"SELECT * FROM gaiaedr3.gaia_source JOIN external.gaiaedr3_distance AS dist USING (source_id)\\\n",
    "    WHERE 1=CONTAINS(POINT({inp_ra}, {inp_dec}),CIRCLE(ra, dec, {radius})) AND \\\n",
    "    ruwe <1.4 AND parallax_over_error >8 AND parallax>0 AND phot_g_mean_mag<20\"\n",
    "\n",
    "    job     = Gaia.launch_job_async(query)\n",
    "    results = job.get_results().to_pandas()\n",
    "\n",
    "    #Delete NANs\n",
    "    results = results.dropna(subset = ['parallax']).reset_index()\n",
    "\n",
    "    data = pd.concat([results['l'],results['b'],results['parallax'],results['pmra'],results['pmdec']],axis=1)\n",
    "    \n",
    "    return data,results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3c30af",
   "metadata": {},
   "source": [
    "\"$\\textit{best_DBSCAN}$\" is used to identify the largest star cluster within that region of space. A broad range of epsilon values must be tested to determine the largest epsilon that results in identifying a single cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc6d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_DBSCAN(data,results):\n",
    "    can_data = 100\n",
    "\n",
    "    epsilon = np.linspace(0.4,0.03,can_data)\n",
    "\n",
    "    cumulos = []\n",
    "    db_value = 0\n",
    "\n",
    "    for j in range(len(epsilon)): #Cada valor de Min_Samples\n",
    "        db = DBSCAN(eps=epsilon[j], min_samples=8).fit(data)\n",
    "        labels = db.labels_\n",
    "\n",
    "        n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        cumulos.append(n_clusters_)  #Cantidad de cúmulos\n",
    "            \n",
    "        if n_clusters_ == 1:\n",
    "            break\n",
    "    \n",
    "    ### Se crea el array de estrellas ###\n",
    "    cluster = data[~labels.astype(bool)]\n",
    "    cluster_cmd = results.iloc[cluster.index.to_numpy()]\n",
    "    cluster_cmd = cluster_cmd.dropna(subset=[\"phot_bp_mean_mag\",\"phot_rp_mean_mag\",\"phot_g_mean_mag\"])\n",
    "    cluster_cmd.drop([\"index\"], axis = 1, inplace = True)\n",
    "    \n",
    "    return cluster_cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722bf175",
   "metadata": {},
   "source": [
    "After applying $\\texttt{DBSCAN}$ and $\\texttt{pyUPMASK}$ to minimize the outliers in the clusters, it is necessary to run a program called \"$\\texttt{fitCMD}$,\" developed in Fortran, to calculate the cluster's isochrone. To properly prepare the data for this process, it must be organized using \"$\\textit{fitCMD_data}$\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dc6032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitCMD_data(cluster_pyUPMASK):\n",
    "    error_g_mean_mag = 2.5/np.log(10)*cluster_pyUPMASK[\"phot_g_mean_flux_error\"]/cluster_pyUPMASK[\"phot_g_mean_flux\"]\n",
    "\n",
    "    error_rp_mean_mag = 2.5/np.log(10)*cluster_pyUPMASK[\"phot_rp_mean_flux_error\"]/cluster_pyUPMASK[\"phot_rp_mean_flux\"]\n",
    "    error_bp_mean_mag = 2.5/np.log(10)*cluster_pyUPMASK[\"phot_bp_mean_flux_error\"]/cluster_pyUPMASK[\"phot_bp_mean_flux\"]\n",
    "\n",
    "    error_bp_rp_color = error_bp_mean_mag + error_rp_mean_mag\n",
    "\n",
    "    error_g_mean_mag = pd.DataFrame(np.transpose(error_g_mean_mag), columns=[\"error_g_mean_mag\"])\n",
    "    error_rp_mean_mag = pd.DataFrame(np.transpose(error_rp_mean_mag), columns=[\"error_rp_mean_mag\"])\n",
    "    error_bp_mean_mag = pd.DataFrame(np.transpose(error_bp_mean_mag), columns=[\"error_bp_mean_mag\"])\n",
    "    error_bp_rp_color = pd.DataFrame(np.transpose(error_bp_rp_color), columns=[\"error_bp_rp_color\"])\n",
    "\n",
    "    cluster_ASteCA = pd.concat([cluster_pyUPMASK,error_g_mean_mag,error_bp_rp_color,error_bp_mean_mag,\n",
    "                                error_rp_mean_mag],axis=1)\n",
    "\n",
    "    cluster_fitCMD= pd.concat([pd.DataFrame(np.arange(1000,1000+len(cluster_pyUPMASK),1), columns=[\"solution_id\"]),\n",
    "                cluster_ASteCA['ra'],cluster_ASteCA['dec'],\n",
    "                cluster_ASteCA['parallax'],cluster_ASteCA['parallax_error'],cluster_ASteCA['pmra'],\n",
    "                cluster_ASteCA['pmra_error'],cluster_ASteCA['pmdec'],cluster_ASteCA['pmdec_error'],\n",
    "                cluster_ASteCA['phot_g_mean_mag'],\n",
    "                cluster_ASteCA['error_g_mean_mag'],\n",
    "                cluster_ASteCA['phot_bp_mean_mag'],\n",
    "                cluster_ASteCA['error_bp_mean_mag'],\n",
    "                cluster_ASteCA['phot_rp_mean_mag'],\n",
    "                cluster_ASteCA['error_rp_mean_mag']],\n",
    "              axis=1)\n",
    "    \n",
    "    cluster_fitCMD.rename(columns = {'solution_id':'#_r', 'ra':'_RAJ2000','dec':'_DEJ2000', 'parallax':\"Plx\", \n",
    "                                    'parallax_error':'e_Plx', 'pmra':'pmRA', 'pmra_error':'e_pmRA','pmdec':'pmDE',\n",
    "                                     'pmdec_error':'e_pmDE', 'phot_g_mean_mag':'Gmag','error_g_mean_mag':'e_Gmag',\n",
    "                                     'phot_bp_mean_mag':'BPmag','error_bp_mean_mag':'e_BPmag', \n",
    "                                     'phot_rp_mean_mag':'RPmag', 'error_rp_mean_mag':'e_RPmag'}, inplace = True)\n",
    "    \n",
    "    return cluster_fitCMD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d696357b",
   "metadata": {},
   "source": [
    "# Code to extract the stars from the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d59d99",
   "metadata": {},
   "source": [
    "The Hunt $\\&$ Refert 2023 catalog is used as the reference catalog. This catalog is essential for identifying clusters located around the positions of those listed within it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618ed3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hunt = pd.read_csv('Archives/Hunt-Refert2023.csv',sep=\",\")\n",
    "repeated_hunt = pd.DataFrame()\n",
    "hunt_clean = hunt.copy()\n",
    "\n",
    "i = np.arange(len(hunt))\n",
    "\n",
    "### Erase duplicate data (clusters that are 1/3600 degrees near to other)\n",
    "\n",
    "while len(i) != 0:\n",
    "    ax = hunt_clean[np.sqrt((hunt_clean.loc[i[0]][\"ra\"]-hunt_clean[\"ra\"])**2+\n",
    "                          (hunt_clean.loc[i[0]][\"dec\"]-hunt_clean[\"dec\"])**2)<=1/3600]\n",
    "    \n",
    "    i = np.setdiff1d(i,ax.index.values)\n",
    "    if len(ax) > 1:\n",
    "        repeated_hunt = pd.concat([ax ,repeated_hunt],axis=0,ignore_index=True)\n",
    "        hunt_clean.drop(labels=ax.index.values[1:],axis=0,inplace=True)\n",
    "\n",
    "hunt_important = hunt_clean[(hunt_clean[\"n_stars\"]<lim2) & (hunt_clean[\"n_stars\"]>lim1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607af796",
   "metadata": {},
   "source": [
    "After importing the Hunt $\\&$ Refert data and removing duplicates, $\\textit{GAIA_search}$ and $\\textit{best_DBSCAN}$ are used to identify the clusters in that region using the methodology established in this project.\n",
    "\n",
    "Additionally, $\\texttt{pyUPMASK}$ is executed to calculate the membership probability for each star in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a370d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(hunt_important))):\n",
    "    clust = hunt_important.iloc[i][\"name\"]\n",
    "    data,results = GAIA_search(clust)\n",
    "    \n",
    "    cluster_cmd = best_DBSCAN(data,results)\n",
    "    print(\"Tamaño del cluster: \", len(cluster_cmd))\n",
    "    \n",
    "    if len(cluster_cmd)>lim1_cmd and len(cluster_cmd)<lim2_cmd:\n",
    "        cluster_cmd.to_csv(path+'/pyUPMASK/input/'+clust+'.csv', index=False)\n",
    "        results.to_csv(path+'/pyUPMASK/results/'+clust+'.csv', index=False)\n",
    "        \n",
    "os.chdir(path+\"/pyUPMASK\")\n",
    "os.system(\"python pyUPMASK.py > pyUPMASK.txt\")\n",
    "os.system(\"rm pyUPMASK.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5ec3a4",
   "metadata": {},
   "source": [
    "Finally, the clusters are processed and prepared for $\\texttt{fitCMD}$ using \"$\\textit{fitCMD_data}$.\" It is necessary to create a separate folder for each cluster, as $\\texttt{fitCMD}$ generates multiple output files. Without proper organization, managing these files can become challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e73d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(path+\"/pyUPMASK/\")\n",
    "Data=os.listdir(\"output\")\n",
    "\n",
    "os.chdir(path)\n",
    "archive= open(\"names.txt\",\"w+\")\n",
    "\n",
    "print(\"\\n \\nFITCMD\\n \\n\")\n",
    "\n",
    "for i in tqdm(range(len(Data))):\n",
    "    clust = Data[i][:-4]\n",
    "    \n",
    "    cluster_pyUPMASK = pd.read_csv(path+\"/pyUPMASK/output/\"+clust+\".csv\",sep=\" \")    \n",
    "    cluster_pyUPMASK = cluster_pyUPMASK[cluster_pyUPMASK[\"probs_final\"]>0.5].reset_index(drop=False)\n",
    "    cluster_fitCMD = fitCMD_data(cluster_pyUPMASK)\n",
    "    \n",
    "    if len(cluster_pyUPMASK)>lim1_cmd and len(cluster_pyUPMASK)<lim2_cmd:\n",
    "        if i<10:\n",
    "            cluster_fitCMD.to_csv(path+'/FitCMD/Data/SL40'+str(i)+'_GAIA.DAT', index=False, sep='\\t',header=True)\n",
    "            archive.write(\"\\ncluster:\"+clust+\" -- Guardado: SL40\"+str(i))\n",
    "        else:\n",
    "            cluster_fitCMD.to_csv(path+'/FitCMD/Data/SL4'+str(i)+'_GAIA.DAT', index=False, sep='\\t',header=True)\n",
    "            archive.write(\"\\ncluster:\"+clust+\" -- Guardado: SL4\"+str(i))\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aba786",
   "metadata": {},
   "source": [
    "# Code to create the catalogue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a4ee4c",
   "metadata": {},
   "source": [
    "First, the dataframe of the complete catalogue is created with the name and astrophysical data obtained by fitCMD.\n",
    "\n",
    "The following steps are dedicated to cleaning the data and selecting the clusters with the relevant columns. Additionally, since fitCMD exports the age of clusters in either Myr or Gyr, it is important to standardize the age column by converting all values to the same unit. The same with distance, kpc and pc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbd8976",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = os.listdir(\"/home/santiago/Documents/Tesis/TesisComparacion/output\")\n",
    "\n",
    "df_SLR=pd.DataFrame(columns=['name','index','Mcl','DM','E','Age','Z','(m-M)o','dSun','AV','E(B-V)','Z/Zsun'])\n",
    "\n",
    "for name in Data:\n",
    "    if name == \"S1161\":\n",
    "        continue\n",
    "    df = pd.read_csv(\"/home/santiago/Documents/Tesis/TesisComparacion/output/\"+name+\"/\"+name,skiprows=[i for i in range(24)]+[29],\n",
    "                skipfooter=32,engine='python',sep=\":\",skipinitialspace = True,names=[\"Parameter\",\"BestComplete\"])\n",
    "\n",
    "    df[\"Parameter\"]=df[\"Parameter\"].str.replace(' ', '')\n",
    "    df[\"BestComplete\"]=df[\"BestComplete\"].str.replace('       ', '_')\n",
    "    df[\"BestComplete\"]=df[\"BestComplete\"].str.replace('      ', '_')\n",
    "    df[\"BestComplete\"]=df[\"BestComplete\"].str.replace('     ', '_')\n",
    "    df[\"BestComplete\"]=df[\"BestComplete\"].str.replace('    ', '_')\n",
    "    df[\"BestComplete\"]=df[\"BestComplete\"].str.replace('   ', '_')\n",
    "    df[\"BestComplete\"]=df[\"BestComplete\"].str.replace('  ', '_')\n",
    "    df[\"BestComplete\"]=df[\"BestComplete\"].str.replace(' ', '')\n",
    "\n",
    "    df[['Best', '+', '-', 'Magnitude']] = df.BestComplete.str.split(\"_\", expand = True)\n",
    "    df = df[['Parameter','Best', '+', '-', 'Magnitude']]\n",
    "    df = df.replace('', pd.NA)\n",
    "    df = df.set_index('Parameter')\n",
    "    df.Best = df.Best.astype(float)\n",
    "    df[\"+\"] = df[\"+\"].astype(float)\n",
    "    df[\"-\"]= df[\"-\"].astype(float)\n",
    "    df = df.T\n",
    "    df[\"name\"] = [name for i in range(len(df))]\n",
    "    df.reset_index(inplace=True)\n",
    "    df = df[['name','index','Mcl','DM','E','Age','Z','(m-M)o','dSun','AV','E(B-V)','Z/Zsun']]\n",
    "    df_SLR = pd.concat([df_SLR,df], ignore_index=True)\n",
    "df_SLR.sort_values(by=['name','index'],ignore_index=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc0ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SLR_Age=df_SLR[(df_SLR['index']=='Best') | (df_SLR['index']=='Magnitude')]\n",
    "index = df_SLR_Age[df_SLR_Age.Age == '(Gyr)'].name\n",
    "best = df_SLR_Age[df_SLR_Age[\"index\"]==\"Best\"]\n",
    "best.loc[best['name'].isin(index),'Age'] = best[best['name'].isin(index)].Age.map(lambda Age: Age*1000)\n",
    "\n",
    "df_SLR_Age = best[[\"name\",\"Mcl\",\"DM\",\"E\",\"Age\",\"Z\",\"(m-M)o\",\"AV\",\"E(B-V)\",\"Z/Zsun\"]]\n",
    "\n",
    "\n",
    "df_SLR_Dist=df_SLR[(df_SLR['index']=='Best') | (df_SLR['index']=='Magnitude')]\n",
    "index = df_SLR_Dist[df_SLR_Dist.dSun == '(kpc)'].name\n",
    "best = df_SLR_Dist[df_SLR_Dist[\"index\"]==\"Best\"]\n",
    "best.loc[best['name'].isin(index),'dSun'] = best[best['name'].isin(index)].dSun.map(lambda dSun: dSun*1000)\n",
    "\n",
    "df_SLR_Dist = best[[\"dSun\"]]\n",
    "\n",
    "df_SLR = df_SLR_Age.join(df_SLR_Dist,how='inner')\n",
    "df_SLR.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1866018a",
   "metadata": {},
   "source": [
    "Another important step is to categorize the clusters as BAD, BADFIT, MEDIUM, or GOOD. This classification depends directly on the CMD. If the CMD is poor, the cluster is labeled as BAD. If the CMD is good but the fitCMD adjustment is poor, it is labeled as BADFIT. If the CMD and the adjustment are both fair, the cluster is categorized as MEDIUM. Finally, when both the CMD and the fit are good, the cluster is labeled as GOOD. This process is made it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd24083",
   "metadata": {},
   "outputs": [],
   "source": [
    "namesGOODBAD = pd.read_csv(\"/home/santiago/codeTesis/codeComplete/FitCMD/badCumules_SLR.txt\",sep=\"-\",names=[\"name\",\"category\"])\n",
    "namesGOODBAD.replace({'GOODWOW':'GOOD'},inplace=True)\n",
    "\n",
    "df_SLR = pd.merge(df_SLR,namesGOODBAD,on=\"name\",how=\"outer\")\n",
    "df_SLR = df_SLR.fillna(\"BAD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508057e2",
   "metadata": {},
   "source": [
    "Add names to the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names = pd.read_csv(\"/home/santiago/Documents/Tesis/TesisComparacion/names.txt\", sep=\" \",names=[\"cluster\",\"name\",\"--\",\"Save\",\"nameSLR\"])\n",
    "df_names = df_names[[\"name\",\"nameSLR\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e98fa1",
   "metadata": {},
   "source": [
    "## Functions of astrophysics coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83490ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode of the data\n",
    "\n",
    "def modaData(phis):\n",
    "    phis = np.array(phis)\n",
    "    phis_ = np.linspace(phis.min(),phis.max(),10000)\n",
    "    \n",
    "    kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.02*(len(phis))**(1/5)).fit(phis[:, np.newaxis])\n",
    "    log_dens = kde.score_samples(phis_[:, np.newaxis])\n",
    "    dens = np.exp(log_dens)\n",
    "\n",
    "    return phis_[np.argmax(dens)]\n",
    "\n",
    "\n",
    "# 50% radius of the data\n",
    "\n",
    "def radio50(df,ra_centro,dec_centro):\n",
    "    df['distancia'] = np.sqrt((df['ra'] - ra_centro)**2 + (df['dec'] - dec_centro)**2)\n",
    "    df = df.sort_values('distancia')\n",
    "    indice_50_por_ciento = round(0.5 * len(df))\n",
    "    radio_50_por_ciento = df.iloc[indice_50_por_ciento]['distancia']\n",
    "    return radio_50_por_ciento\n",
    "\n",
    "# Radius for half of the luminosity\n",
    "\n",
    "def radio50_lum(df,ra_centro,dec_centro):\n",
    "    df=df.copy()\n",
    "    df['distancia'] = np.sqrt((df['ra'] - ra_centro)**2 + (df['dec'] - dec_centro)**2) #Distancie to the center\n",
    "    df = df.sort_values('distancia').reset_index(drop=True) #They are sorted from nearest to furthest.\n",
    "    half_light = sum(df['phot_g_mean_mag'])/2 #The value for half of the luminosity.\n",
    "    \n",
    "    df['cumsum'] = df['phot_g_mean_mag'].cumsum() #Once the indices are sorted, a cumulative sum is performed.\n",
    "    distance = df[df[\"cumsum\"] <= half_light].tail(1)[\"distancia\"].values[0] #When the sum reaches half_light.\n",
    "    \n",
    "    return distance\n",
    "\n",
    "# Error to the right of the data (84%)\n",
    "\n",
    "def error_84(array):\n",
    "    media = array.mean()\n",
    "    maximo = array.max()\n",
    "    \n",
    "    array_84 = np.linspace(media,maximo,1000)\n",
    "    \n",
    "    for value in array_84:\n",
    "        quantity = len(array[array <= value])/len(array)\n",
    "        if quantity >= 0.84:\n",
    "            return value\n",
    "            \n",
    "# Error to the left of the data (16%)\n",
    "\n",
    "def error_16(array):\n",
    "    media = array.mean()\n",
    "    minimo = array.min()\n",
    "    \n",
    "    array_16 = np.linspace(minimo,media,1000)\n",
    "    \n",
    "    for value in array_16:\n",
    "        quantity = len(array[array <= value])/len(array)\n",
    "        if quantity >= 0.16:\n",
    "            return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce627cf1",
   "metadata": {},
   "source": [
    "Creation of the complete catalogue with all the astrophyscis coords and errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b984cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = pd.unique(StarsGoodMedium.nameSLR)\n",
    "coordinates = pd.DataFrame()\n",
    "\n",
    "for name in tqdm(names):\n",
    "    df= pd.DataFrame()\n",
    "    df[\"name\"] = [name]\n",
    "    df[\"nStars\"] = [len(StarsGoodMedium[StarsGoodMedium.nameSLR == name])]\n",
    "    \n",
    "    df[\"ra\"] = [modaData(StarsGoodMedium[StarsGoodMedium.nameSLR == name].ra)]\n",
    "    df[\"ra_16\"] = [error_16(StarsGoodMedium[StarsGoodMedium.nameSLR == name].ra)]\n",
    "    df[\"ra_84\"] = [error_84(StarsGoodMedium[StarsGoodMedium.nameSLR == name].ra)]\n",
    "    \n",
    "    df['dec'] = [modaData(StarsGoodMedium[StarsGoodMedium.nameSLR == name].dec)]\n",
    "    df[\"dec_16\"] = [error_16(StarsGoodMedium[StarsGoodMedium.nameSLR == name].dec)]\n",
    "    df[\"dec_84\"] = [error_84(StarsGoodMedium[StarsGoodMedium.nameSLR == name].dec)]\n",
    "    \n",
    "    df['parallax'] = [modaData(StarsGoodMedium[StarsGoodMedium.nameSLR == name].parallax)]\n",
    "    df[\"parallax_16\"] = [error_16(StarsGoodMedium[StarsGoodMedium.nameSLR == name].parallax)]\n",
    "    df[\"parallax_84\"] = [error_84(StarsGoodMedium[StarsGoodMedium.nameSLR == name].parallax)]\n",
    "    \n",
    "    df['pmra'] = [modaData(StarsGoodMedium[StarsGoodMedium.nameSLR == name].pmra)]\n",
    "    df[\"pmra_16\"] = [error_16(StarsGoodMedium[StarsGoodMedium.nameSLR == name].pmra)]\n",
    "    df[\"pmra_84\"] = [error_84(StarsGoodMedium[StarsGoodMedium.nameSLR == name].pmra)]    \n",
    "    \n",
    "    df['pmdec'] = [modaData(StarsGoodMedium[StarsGoodMedium.nameSLR == name].pmdec)]\n",
    "    df[\"pmdec_16\"] = [error_16(StarsGoodMedium[StarsGoodMedium.nameSLR == name].pmdec)]\n",
    "    df[\"pmdec_84\"] = [error_84(StarsGoodMedium[StarsGoodMedium.nameSLR == name].pmdec)]    \n",
    "    \n",
    "    df['l'] = [modaData(StarsGoodMedium[StarsGoodMedium.nameSLR == name].l)]\n",
    "    df[\"l_16\"] = [error_16(StarsGoodMedium[StarsGoodMedium.nameSLR == name].l)]\n",
    "    df[\"l_84\"] = [error_84(StarsGoodMedium[StarsGoodMedium.nameSLR == name].l)]  \n",
    "    \n",
    "    df['b'] = [modaData(StarsGoodMedium[StarsGoodMedium.nameSLR == name].b)]\n",
    "    df[\"b_16\"] = [error_16(StarsGoodMedium[StarsGoodMedium.nameSLR == name].b)]\n",
    "    df[\"b_84\"] = [error_84(StarsGoodMedium[StarsGoodMedium.nameSLR == name].b)]   \n",
    "    \n",
    "    df['r50'] = [radio50(StarsGoodMedium[StarsGoodMedium.nameSLR == name][[\"ra\",\"dec\"]],\n",
    "                         df.ra.values[0],df.dec.values[0])]\n",
    "    \n",
    "    df['rhl'] = [radio50_lum(StarsGoodMedium[StarsGoodMedium.nameSLR == name],\n",
    "                             df.ra.values[0],df.dec.values[0])]\n",
    "    \n",
    "    coordinates = pd.concat([coordinates,df],ignore_index=True,axis=0)\n",
    "    \n",
    "df_SLR = pd.merge(coordinates,df_SLR,how=\"inner\",on='name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ee4fba",
   "metadata": {},
   "source": [
    "Also add X,Y,Z coords from SkyCoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dca210",
   "metadata": {},
   "outputs": [],
   "source": [
    "distancia = df_SLR[\"dSun\"].astype(float).values * u.pc  # Por ejemplo, 100 parsecs\n",
    "longitud = df_SLR[\"l\"] * u.deg   # Longitud galáctica\n",
    "latitud = df_SLR[\"b\"] * u.deg    # Latitud galáctica\n",
    "\n",
    "# Crear un objeto SkyCoord con las coordenadas galácticas\n",
    "coordenadas_galacticas = SkyCoord(l=longitud, b=latitud, distance=distancia, frame='galactic')\n",
    "\n",
    "# Obtener las coordenadas cartesianas x, y, z\n",
    "df_SLR[\"X\"] = pd.DataFrame(coordenadas_galacticas.cartesian.x, columns=[\"X\"])\n",
    "df_SLR[\"Y\"] = pd.DataFrame(coordenadas_galacticas.cartesian.y, columns=[\"Y\"])\n",
    "df_SLR[\"Z\"] = pd.DataFrame(coordenadas_galacticas.cartesian.z, columns=[\"Z\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016c5f10",
   "metadata": {},
   "source": [
    "## Additionally, create a DataFrame containing all the stars within the clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bf508c",
   "metadata": {},
   "source": [
    "All the stars are imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd9ead7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stars_Data = os.listdir(\"/home/santiago/codeTesis/codeComplete/pyUPMASK/RunsCode/Total_output/\")\n",
    "\n",
    "Stars=pd.DataFrame()\n",
    "\n",
    "for name in Stars_Data:\n",
    "    df = pd.read_csv(\"/home/santiago/codeTesis/codeComplete/pyUPMASK/RunsCode/Total_output/\"+name, sep=\" \",header=0)\n",
    "    df[\"name\"] = [name[:-4]]*len(df)\n",
    "    Stars = pd.concat([Stars,df],ignore_index=True,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f4ede",
   "metadata": {},
   "source": [
    "The categories of the clusters are analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcd1447",
   "metadata": {},
   "outputs": [],
   "source": [
    "goodMedium = pd.merge(namesGOODBAD,df_names,how=\"left\",left_on='name', right_on='nameSLR')\n",
    "goodMedium = goodMedium[[\"name_x\",\"name_y\",\"category\"]]\n",
    "goodMedium = goodMedium.rename(columns={\"name_x\":\"nameSLR\",\"name_y\":\"name\"})\n",
    "goodMedium = goodMedium[(goodMedium.category == \"GOOD\") | (goodMedium.category == \"MEDIUM\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f097d0",
   "metadata": {},
   "source": [
    "Stars classified as GOOD or MEDIUM are selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b71ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "StarsGoodMedium = Stars[Stars.name.isin(goodMedium.name)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c144051",
   "metadata": {},
   "source": [
    "A join is performed with nameSLR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f5261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "StarsGoodMedium = pd.merge(StarsGoodMedium,df_names,how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304db3cb",
   "metadata": {},
   "source": [
    "### Export catalogue and stars from catalogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a49fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_SLR.to_csv(\"SLR_catalogue_570.csv\",sep=\";\",index=False)\n",
    "StarsGoodMedium.to_csv(\"SLR_members.csv\",sep=\";\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
